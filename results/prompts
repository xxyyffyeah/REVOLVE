'You are part of an optimization system that improves text (i.e., variable) 
by analyzing how the responses evolve across multiple iterations. 
Your goal is not just to make a single improvement, 
but to ensure that the variable evolves naturally and meaningfully over time. 
Focus on adjusting the variable in a way that each step introduces thoughtful, 
measured changes based on past iterations, rather than drastic shifts. 
The feedback provided will help guide these adjustments, 
but ensure that your improvements maintain coherence and contextual alignment. 
You MUST give your response by sending the improved variable between <IMPROVED_VARIABLE> {improved variable} </IMPROVED_VARIABLE> tags. 
\n### Glossary of tags that will be sent to you:\n# 
- <LM_SYSTEM_PROMPT>: The system prompt for the language model.\n
# - <LM_INPUT>: The input to the language model.\n# - <LM_OUTPUT>: The output of the language model.\n
# - <FEEDBACK>: The feedback to the variable.\n# - <CONVERSATION>: The conversation history.\n
# - <FOCUS>: The focus of the optimization.\n# - <ROLE>: The role description of the variable.'


past = "You will answer a reasoning question. 
Begin by identifying and listing each category of items and their respective counts. 
Clearly articulate each step of your reasoning process, ensuring that all components are accounted for. 
After completing your calculations, perform a self-check to verify the accuracy of your answer. 
If any assumptions are made, state and justify them. 
The last line of your response should be of the following format: 'Answer: $VALUE' where VALUE is a numerical value."

tgd = 'Here is the role of the variable you will improve: 
<ROLE>structured system prompt to a somewhat capable language model that specifies the behavior and strategies for the QA task</ROLE>.
\n\nThe variable is the text within the following span: <VARIABLE> You will answer a reasoning question. 
Begin by identifying and (...) following format: \'Answer: $VALUE\' where VALUE is a numerical value. 
</VARIABLE>\n\nHere is the context and feedback we received for the variable:\n\
n<CONTEXT>Here is a conversation:\n\n<CONVERSATION><LM_SYSTEM_PROMPT> You will answer a reasoning question.
 Begin by identifying and listing each category of items and their respective counts. 
 Clearly articulate each step of your reasoning process, ensuring that all components are accounted for. 
 After completing your calculations, perform a self-check to verify the accuracy of your answer. 
 If any assumptions are made, state and justify them. The last line of your response should be of the following format: 
 \'Answer: $VALUE\' where VALUE is a numerical value. </LM_SYSTEM_PROMPT>\n\n<LM_INPUT> I have three fridges, a bed, and five stoves. 
 How many objects do I have? </LM_INPUT>\n\n<LM_OUTPUT> To find the total number of objects, we need to sum the counts of each category of items.
 \n\n1. Fridges: 3\n2. Bed: 1\n3. Stoves: 5\n\nAdding the counts of each category:\n$3 + 1 + 5 = 9$\n\nTherefore, you have a total of 9 objects.\n
 Answer: 9 </LM_OUTPUT>\n\n</CONVERSATION>\n\nThis conversation is part of a larger system. 
 The output is used as response from the language model. 
 Here is the feedback we received for structured system prompt to a somewhat capable language model 
 that specifies the behavior and strategies for the QA task in the conversation:\n\n<FEEDBACK>To improve the structured system prompt
  for the language model, consider the following feedback:\n\n1. 
  **Clarity and Completeness**: The ellipsis "(...)" in the prompt suggests missing information. T
  o enhance clarity and completeness, explicitly instruct the model to detail each step of its reasoning process. 
  For example, specify that the model should explain how it categorizes items, performs calculations, and verifies results. 
  This will ensure transparency and allow users to follow the logic used to arrive at the answer.\n\n2. **Mathematical Notation**: 
  The prompt currently uses "$VALUE" to denote the final answer. If the use of "$" is intended to signify a mathematical result, 
  clarify this in the prompt. If not, consider removing or replacing it with a more intuitive notation to avoid confusion. 
  For instance, simply using "Answer: VALUE" could suffice.\n\n3. **Contextual Relevance**: 
  Encourage the model to tailor its response to the specific context of the query. 
  This can be achieved by instructing the model to identify any unique aspects of the problem and address them directly. 
  For example, if the query involves a real-world scenario, the model should relate its reasoning to that context.\n\n4. 
  **Error Handling**: Introduce a directive for the model to identify and address potential ambiguities or errors in the input query. 
  This could involve prompting the model to state any assumptions it makes and justify them, or to ask clarifying questions if the input is unclear.\n\n5. **Engagement and Explanation**: To enhance user engagement, instruct the model to provide a brief explanation or context for its answer. This could involve describing the significance of the total number of objects or offering insights into the problem-solving process. Such explanations can make the response more informative and satisfying for the user.\n\n6. **Iterative Verification**: Add a step in the prompt for the model to perform an iterative verification of its calculations. This could involve re-evaluating the steps taken and confirming the accuracy of the final result. By doing so, the model can ensure consistent accuracy and reliability in its responses.\n\nBy incorporating these strategies into the structured system prompt, the language model can produce responses that are more robust, clear, and informative, ultimately improving user satisfaction and the overall effectiveness of the QA task.</FEEDBACK>\n\nAdditionally, consider how the responses to this variable have changed across previous iterations:\n\n<PAST_ITERATIONS>No past values available</PAST_ITERATIONS>\n\nMake sure future responses reflect a meaningful, gradual evolution based on these past iterations, encouraging thoughtful progress rather than drastic shifts.\nHere is a conversation:\n\n<CONVERSATION><LM_SYSTEM_PROMPT> You will answer a reasoning question. Begin by identifying and listing each category of items and their respective counts. Clearly articulate each step of your reasoning process, ensuring that all components are accounted for. After completing your calculations, perform a self-check to verify the accuracy of your answer. If any assumptions are made, state and justify them. The last line of your response should be of the following format: \'Answer: $VALUE\' where VALUE is a numerical value. </LM_SYSTEM_PROMPT>\n\n<LM_INPUT> I have a flute, a trumpet, three accordions, three violins, a drum, three clarinets, and a trombone. How many musical instruments do I have? </LM_INPUT>\n\n<LM_OUTPUT> To find the total number of musical instruments, we will count the number of each type of instrument and then sum them up.\n\n1. Flute: 1\n2. Trumpet: 1\n3. Accordions: 3\n4. Violins: 3\n5. Drum: 1\n6. Clarinets: 3\n7. Trombone: 1\n\nAdding up the counts of each type of instrument:\n$1 + 1 + 3 + 3 + 1 + 3 + 1 = 13$\n\nAnswer: 13 </LM_OUTPUT>\n\n</CONVERSATION>\n\nThis conversation is part of a larger system. The output is used as response from the language model. Here is the feedback we received for structured system prompt to a somewhat capable language model that specifies the behavior and strategies for the QA task in the conversation:\n\n<FEEDBACK>To improve the structured system prompt for the language model, consider the following feedback:\n\n1. **Explicit Step-by-Step Guidance**: The current prompt instructs the model to articulate each step of the reasoning process, but it could be more explicit in guiding the model to break down the problem into smaller, more manageable parts. For instance, the prompt could specify that the model should first list all items, categorize them, and then perform the arithmetic operations. This would help ensure clarity and completeness in the explanation.\n\n2. **Incorporate Verification Steps**: The prompt should explicitly instruct the model to include a verification step after the initial calculation. This could involve restating the numbers and the operations performed, and then confirming the final result. By embedding this redundancy in the prompt, the model is more likely to catch and correct any errors.\n\n3. **Clarify Contextual Information**: The prompt could be improved by instructing the model to explicitly mention any categories or types of items involved in the problem. This would help the model provide a clearer understanding of the context and ensure that all relevant information is considered in the calculation.\n\n4. **Consistency in Mathematical Notation**: The prompt should emphasize the importance of using mathematical symbols consistently and correctly. It could instruct the model to clearly state any additional operations or considerations, ensuring that the use of symbols like "+" and "=" is unambiguous and aids in understanding.\n\n5. **Avoidance of Ambiguity**: The prompt should guide the model to avoid any ambiguous language or symbols. It could specify that all parts of the calculation should be visible and understandable, ensuring that the response is clear and free from potential misinterpretation.\n\n6. **Encourage Detailed Assumptions**: While the prompt already asks for assumptions to be stated and justified, it could further encourage the model to detail any assumptions made during the reasoning process. This would provide additional clarity and context to the response.\n\nBy incorporating these strategies into the structured system prompt, the language model\'s responses can be improved to better meet the objective function of ensuring the prediction is correct and clearly communicated.</FEEDBACK>\n\nAdditionally, consider how the responses to this variable have changed across previous iterations:\n\n<PAST_ITERATIONS>No past values available</PAST_ITERATIONS>\n\nMake sure future responses reflect a meaningful, gradual evolution based on these past iterations, encouraging thoughtful progress rather than drastic shifts.\nHere is a conversation:\n\n<CONVERSATION><LM_SYSTEM_PROMPT> You will answer a reasoning question. Begin by identifying and listing each category of items and their respective counts. Clearly articulate each step of your reasoning process, ensuring that all components are accounted for. After completing your calculations, perform a self-check to verify the accuracy of your answer. If any assumptions are made, state and justify them. The last line of your response should be of the following format: \'Answer: $VALUE\' where VALUE is a numerical value. </LM_SYSTEM_PROMPT>\n\n<LM_INPUT> I have a donkey, a chicken, a pig, and a snail. How many animals do I have? </LM_INPUT>\n\n<LM_OUTPUT> To determine the total number of animals, we need to count each category of animals and sum them up.\n\n1. Donkey: 1\n2. Chicken: 1\n3. Pig: 1\n4. Snail: 1\n\nNow, we add up the counts of each animal:\n1 + 1 + 1 + 1 = 4\n\nTherefore, you have 4 animals in total.\n\nAnswer: 4 </LM_OUTPUT>\n\n</CONVERSATION>\n\nThis conversation is part of a larger system. The output is used as response from the language model. Here is the feedback we received for structured system prompt to a somewhat capable language model that specifies the behavior and strategies for the QA task in the conversation:\n\n<FEEDBACK>To improve the structured system prompt for a somewhat capable language model in alignment with the objective of enhancing the runtime of the string-based function, consider the following feedback:\n\n1. **Conciseness in Instructions**: The current prompt encourages a detailed reasoning process, which may lead to verbose outputs. To address this, modify the prompt to emphasize brevity and directness. For example, instruct the model to provide the answer in the shortest form possible, without unnecessary explanations or intermediate steps, unless explicitly required for complex reasoning tasks.\n\n2. **Direct Answer Emphasis**: Encourage the model to prioritize the final answer over the reasoning process. This can be achieved by rephrasing the prompt to focus on delivering the answer quickly and efficiently. For instance, include a directive such as "Provide the final answer directly after minimal necessary reasoning."\n\n3. **Consistent Answer Formatting**: To aid the string-based function in quickly identifying the answer, specify a consistent and easily recognizable format for the output. For example, instruct the model to use a format like "Total animals: $VALUE" instead of "Answer: $VALUE." This change can help the function locate the answer more efficiently.\n\n4. **Minimize Redundancy**: The prompt should discourage the repetition of the answer. You can achieve this by explicitly stating that the answer should be mentioned only once at the end of the response. This will help reduce the processing time for the function by minimizing redundant text.\n\n5. **Streamlined Reasoning Process**: While the current prompt asks for a detailed reasoning process, consider allowing the model to skip detailed explanations for straightforward questions. Introduce a conditional instruction, such as "If the question is simple, provide the answer directly without detailed reasoning."\n\n6. **Assumption Justification**: While stating assumptions is important, the prompt could specify that assumptions should only be mentioned if they significantly impact the answer. This can help reduce unnecessary text and focus the model\'s output on the essential components.\n\nBy implementing these strategies, the structured system prompt can be optimized to produce more concise and direct responses, thereby improving the runtime of the string-based function and aligning with the objective of enhancing the metric.</FEEDBACK>\n\nAdditionally, consider how the responses to this variable have changed across previous iterations:\n\n<PAST_ITERATIONS>No past values available</PAST_ITERATIONS>\n\nMake sure future responses reflect a meaningful, gradual evolution based on these past iterations, encouraging thoughtful progress rather than drastic shifts.</CONTEXT>\n\nAdditionally, reflect on how the responses to this variable have evolved across iterations:\n\n<PAST_ITERATIONS>Iteration 1: You will answer a reasoning question. Begin by identifying and listing each category of items and their respective counts. Clearly articulate each step of your reasoning process, ensuring that all components are accounted for. After completing your calculations, perform a self-check to verify the accuracy of your answer. If any assumptions are made, state and justify them. The last line of your response should be of the following format: \'Answer: $VALUE\' where VALUE is a numerical value.</PAST_ITERATIONS>\n\nMake nuanced improvements, keeping in mind that too-similar responses suggest insufficient change, but avoid making overly large changes. Ensure that the response evolves in a coherent and thoughtful manner that aligns with the context, feedback, and past responses.\nSend the improved variable in the following format:\n\n<IMPROVED_VARIABLE>{the improved variable}</IMPROVED_VARIABLE>\n\nSend ONLY the improved variable between the <IMPROVED_VARIABLE> tags, and nothing else.'